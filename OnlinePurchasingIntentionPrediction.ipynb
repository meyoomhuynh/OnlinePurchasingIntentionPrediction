{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b317b085",
   "metadata": {},
   "source": [
    "# Online Purchasing Intention Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba815f11",
   "metadata": {},
   "source": [
    "#### Import the potential libraries to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c993eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#For Random Forest - Prediction\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Decision Tree - Interpretation\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(862)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318f0ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "# In the original data, there is 12,330 observations and 18 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854a9a0",
   "metadata": {},
   "source": [
    "**Ultimate Goal- Productionize of the Project**: The model can be used in marketing team to build strategy on which features of a visitor to target future advertisements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f080ba",
   "metadata": {},
   "source": [
    "## Part 1: Data Pre-proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52383d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the Month column includes all 12 months. It's missing Jan and Apr.\n",
    "df.Month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the population of each Visitor Type. \n",
    "df.groupby('VisitorType')['Revenue'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 85 of the visitors are not categorized. We decided to removed all \"Other\" Visitor Type.\n",
    "df.drop(df[df['VisitorType'] == 'Other'].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5006473",
   "metadata": {},
   "source": [
    "## Part 2: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4435c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for correlations between features\n",
    "plt.figure(figsize = (20,10))\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=bool))\n",
    "heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3084c45",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "* Adminstrative, Informational, ProductRelated, Exit rate and Page Values show **high correlation** values.\n",
    "* Drop ExitRate and use only BounceRate because BounceRate of a page **explains the probability** that a visitor exits \n",
    "the browsing session after viewing that page. It suggests how effective that page is in convincing the visitor to stay longer. Since our project towards more retail shopping and getting the purchase, it signifies more on **first impression** on the pages.\n",
    "* Our team also decides to **not use** Page Values in our models because it is a feature that generate after a person is done purchased. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abca9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the scope of our project, we will only use the following features in our models.\n",
    "df_2 = df[['Administrative_Duration', 'Informational_Duration',\n",
    "       'ProductRelated_Duration', 'BounceRates', 'SpecialDay', 'Month', 'VisitorType', 'Weekend',\n",
    "        'Revenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "df_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7adc98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the data using seaborn Pairplots\n",
    "g = sns.pairplot(df_2, hue = 'Revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d830e20",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "* How did we choose our method? We have these qualifications to think about: Supervised Learning, Classification problem with overlap observations. We want to have one model with high interpretation to answer our ultimate managerial question. But we also want to provide the marketing team a prediction model that they can use for direct prediction. \n",
    "\n",
    "\n",
    "* Looking at the pairplot above: it is clear that thereâ€™s lots of overlap in many variables. This means logistic regression is probably not a good choice. So we have Decision Tree, Random Forest, XGBoost, SVM or a combination of these in ensemble, or also KNN, however KNN is known for very slow execution. Our team will use Decision Tree (Interpretation), Random Forest to compare our models and also Ensemble Learning with a base of XGBoost, SVM and a blender of KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379da027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Investigate all the numerical features by our y. Density plots show the distribution of all the numberical features.\n",
    "num_features = ['Administrative_Duration', 'Informational_Duration',\n",
    "       'ProductRelated_Duration', 'BounceRates', 'SpecialDay']\n",
    "\n",
    "for f in num_features:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.xlabel(f)\n",
    "    plt.ylabel('Revenue')\n",
    "    sns.kdeplot(df_2[f],fill=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd6b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Investigate all the categorical features by our y.\n",
    "cat_features = ['Month', 'VisitorType', 'Weekend']\n",
    "\n",
    "for f in cat_features:\n",
    "    plt.figure()\n",
    "    ax = sns.countplot(x=f, data=df_2, hue = 'Revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ce5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/31749448/how-to-add-percentages-on-top-of-grouped-bars\n",
    "\n",
    "def with_hue(ax, feature, Number_of_categories, hue_categories):\n",
    "    a = [p.get_height() for p in ax.patches]\n",
    "    patch = [p for p in ax.patches]\n",
    "    for i in range(Number_of_categories):\n",
    "        total = feature.value_counts().values[i]\n",
    "        for j in range(hue_categories):\n",
    "            percentage = '{:.1f}%'.format(100 * a[(j*Number_of_categories + i)]/total)\n",
    "            x = patch[(j*Number_of_categories + i)].get_x() + patch[(j*Number_of_categories + i)].get_width() / 2 - 0.15\n",
    "            y = patch[(j*Number_of_categories + i)].get_y() + patch[(j*Number_of_categories + i)].get_height() \n",
    "            ax.annotate(percentage, (x, y), size = 12)\n",
    "\n",
    "def without_hue(ax, feature):\n",
    "    total = len(feature)\n",
    "    for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "        x = p.get_x() + p.get_width() / 2 - 0.05\n",
    "        y = p.get_y() + p.get_height()\n",
    "        ax.annotate(percentage, (x, y), size = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e09029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot percentage of purchase by visitor types.\n",
    "\n",
    "plt.figure (figsize =(7,5))\n",
    "ax = sns.countplot(x='VisitorType', data=df_2, hue = 'Revenue')\n",
    "plt.xticks(size=12)\n",
    "plt.xlabel ('Revenue')\n",
    "plt.yticks(size=12)\n",
    "plt.ylabel ('count',size=12)\n",
    "\n",
    "with_hue(ax,df_2.VisitorType,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75117e7d",
   "metadata": {},
   "source": [
    "**Notes from graph: percentage of purchase by visitor types:** From our EDA, we saw that out of all the returning visitors, there are only 14% that decided to purchase. Out of all the new visitors, almost 25% of them purchase more! Which leads us to our main hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e53c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for Outliers\n",
    "\n",
    "fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2) \n",
    "ax1.boxplot(df_2.Administrative_Duration)\n",
    "ax2.boxplot(df_2.Informational_Duration)\n",
    "ax3.boxplot(df_2.ProductRelated_Duration)\n",
    "ax4.boxplot(df_2.BounceRates)\n",
    "\n",
    "ax1.title.set_text('Administrative_Duration')\n",
    "ax2.title.set_text('Informational_Duration')\n",
    "ax3.title.set_text('ProductRelated_Duration')\n",
    "ax4.title.set_text('BounceRates')\n",
    "\n",
    "plt.suptitle('Data Outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the range of the variable and the count of each value. Outliers start from value over 2200.\n",
    "df_2.groupby('Administrative_Duration')['Revenue'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2be7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers of Administrative_Duration over 2200.\n",
    "df_2.drop(df_2[df_2['Administrative_Duration'] >= 2200].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6e447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for the range of the variable and the count of each value. Outliers start from value over 20000.\n",
    "df_2.groupby('ProductRelated_Duration')['Revenue'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3845163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers of ProductRelated_Duration over 20000.\n",
    "df_2.drop(df_2[df_2['ProductRelated_Duration'] >= 20000].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc20a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for Month and Visitor features\n",
    "df_2 = pd.get_dummies(data = df_2, columns = ['VisitorType','Month'], drop_first = True)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d42142",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f0dfd",
   "metadata": {},
   "source": [
    "## Part 3: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a0242",
   "metadata": {},
   "source": [
    "### 1) Hypothesis Testing Using Chi-Squared Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28f068",
   "metadata": {},
   "source": [
    "During our EDA process, looks like New visitors have higher purchasing intention than Returning Visitors. We would like to test if if finding is statistically significant. First, we check if there is a relationship between the type of visitors and their purchasing intention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4c254",
   "metadata": {},
   "source": [
    "* **H0a:** there is **no** relationship between type of visitor and purchasing intention\n",
    "* **H1a:** there is **statistically significant relationship** between type of visitor and purchasing intention\n",
    "\n",
    "Source Code: https://github.com/yug95/MachineLearning/blob/master/Hypothesis%20testing/Paired%20T-test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8285f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce29eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df[['VisitorType','Revenue']]\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52252b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contigency table\n",
    "contingency_table=pd.crosstab(df_3[\"VisitorType\"],df_3[\"Revenue\"])\n",
    "print('Contingency_table :\\n',contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bf7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed Values\n",
    "Observed_Values = contingency_table.values \n",
    "print(\"Observed Values :\\n\",Observed_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Values\n",
    "b=stats.chi2_contingency(contingency_table)\n",
    "Expected_Values = b[3]\n",
    "print(\"Expected Values :\\n\",Expected_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Degree of Freedom\n",
    "no_of_rows=len(contingency_table.iloc[0:2,0])\n",
    "no_of_columns=len(contingency_table.iloc[0,0:2])\n",
    "df_3=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",df_3)\n",
    "\n",
    "# Set significant level alpha at 0.05.\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the p-value of the Chi-Squared Test\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_Values,Expected_Values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "p_value= 1-chi2.cdf(x=chi_square_statistic, df=df_3)\n",
    "print('P-value of the Chi-Squared Test is:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Critical Value\n",
    "critical_value=chi2.ppf(q=1-alpha,df=df_3)\n",
    "print('critical_value:',critical_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=df_3)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce854d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if p_value<=alpha:\n",
    "    print(\"We reject Null Hypothesis H0 as p-value is smaller than alpha.\"\n",
    "          \" There is a relationship between between the type of visitor and purchasing intention.\")\n",
    "else:\n",
    "    print(\"We failed to reject Null Hypothesis H0 as p-value is greater than alpha.\"\n",
    "          \" There is no relationship between between type of visitor and purchasing intention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c9c64",
   "metadata": {},
   "source": [
    "### 2) Hypothesis Testing Using T-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f37279",
   "metadata": {},
   "source": [
    "We have validated that there is a statistically significant relationship between the type of visitor and purchasing intention. We would like to further test if the ReturningVisitor has different purchasing rate than NewVisitor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05def1",
   "metadata": {},
   "source": [
    "* **H0b:** Purchasing Rate of Returning Visitor is the **same** as Purchasing Rate of New Visitors\n",
    "* **H1b**: Purchasing Rate of Returning Visitor is **different** than Purchasing Rate of New Visitors\n",
    "\n",
    "Source Code: https://www.pythonfordatascience.org/independent-samples-t-test-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77488cf",
   "metadata": {},
   "source": [
    "#### Using researchpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c523d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import researchpy as rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df[['VisitorType','Revenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd405c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, results = rp.ttest(group1= df_4['Revenue'][df_4['VisitorType'] == 'Returning_Visitor'], group1_name= \"Returning_Visitors\",\n",
    "                            group2= df_4['Revenue'][df_4['VisitorType'] == 'New_Visitor'], group2_name= \"New_Visitors\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d6060",
   "metadata": {},
   "source": [
    "#### Using scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind(df_4['Revenue'][df_4['VisitorType'] == 'Returning_Visitor'],\n",
    "                df_4['Revenue'][df_4['VisitorType'] == 'New_Visitor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446d142",
   "metadata": {},
   "source": [
    "**Conclusion:** Since p-value is significant, we reject the H0. The average purchasing probability of Returning Visitor is statistically significantly different than of New Visitors, and is lower than of New Visitor (0.1393 < 0.2491); t(12243)=-11.67, p-value = 0.0000000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c7071",
   "metadata": {},
   "source": [
    "## Part 4: Predictive Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0dd16",
   "metadata": {},
   "source": [
    "### 1) Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb27e83",
   "metadata": {},
   "source": [
    "Recall that we are building a model to answer the question:\n",
    "What strategies marketing team should use for advertisement targeting to increase purchase rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59691d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ef8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y for Decision Tree Model\n",
    "X = df_2.drop('Revenue', axis = 1)\n",
    "y = df_2.Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34506275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split on dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=862)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee09f2",
   "metadata": {},
   "source": [
    "#### Step 1: Tune Decision Tree Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52535b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GridSearchCV\n",
    "dt = DecisionTreeClassifier()\n",
    "param = {'criterion':['entropy'],\n",
    "         'max_depth':range(2,17),\n",
    "         'min_samples_split':[3, 5, 10, 15],\n",
    "         'min_samples_leaf':[3, 5, 10, 15]}\n",
    "\n",
    "\n",
    "grid= GridSearchCV(dt, param, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffcbf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fitting decision tree\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fb888",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters:\",grid.best_params_)\n",
    "print(\"Accuracy of model:\",np.mean(grid.predict(X_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9e8eb",
   "metadata": {},
   "source": [
    "#### Step 2: Apply tuned hyperparameters to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf88486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64cf7ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Because the best max depth is 3, which is too small to understand the tree. I will increase it \n",
    "# to 4 and re-run the tree. \n",
    "dt_tuned = DecisionTreeClassifier(criterion = \"entropy\",\n",
    "                                  max_depth = 4,\n",
    "                                  min_samples_leaf = 3,\n",
    "                                  min_samples_split = 3,\n",
    "                                  random_state = 862)\n",
    "\n",
    "dt_tuned.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2e540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the important features\n",
    "Feature_Importance = pd.DataFrame({'feature':X.columns.values, 'importance':dt_tuned.feature_importances_})\n",
    "Feature_Importance.sort_values(by = ['importance'], ascending = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05bd29e",
   "metadata": {},
   "source": [
    "#### Step 3: Visualize Decision Tree for Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3235b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(\"Decision Tree\", figsize = [25,8])\n",
    "plot_tree(dt_tuned,fontsize=10, filled=True, feature_names=X.columns, class_names = True)\n",
    "plt.tight_layout()\n",
    "plt.title(\"Decision Tree\", size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# You can call predict the same way we called predict to any other algorithm we used before\n",
    "y_hat = dt_tuned.predict(X_test)\n",
    "print(y_hat)\n",
    "\n",
    "#Prediction confusion matrix:\n",
    "print(\"Results from the Decision Tree Classifier:\\n\")\n",
    "cf_matrix = confusion_matrix(y_test, y_hat)\n",
    "cf_matrix\n",
    "\n",
    "TN = cf_matrix[0,0] #True Negative\n",
    "FN = cf_matrix[1,0] #False Negative\n",
    "FP = cf_matrix[0,1] #False Positive\n",
    "TP = cf_matrix[1,1] #True Positive\n",
    "\n",
    "Err = float(FP + FN)/(FP + FN + TP + TN) #Prediction Error\n",
    "Acc = float(TP + TN)/(FP + FN + TP + TN) #Prediction Accuracy\n",
    "FPR = float(FP)/(FP + TN)  #False Positive Rate\n",
    "TNR = float(TN)/(FP + TN)  #True Negative Rate\n",
    "TPR = float(TP)/(FN + TP)  #True Positive Rate\n",
    "FNR = float(FN)/(FN + TP)  #False Negative Rate\n",
    "print(\"False Positive Rate = %f \" %FPR)\n",
    "print(\"False Negative Rate = %f \" %FNR)\n",
    "print(\"True Positive Rate = %f \" %TPR)\n",
    "print(\"True Negative Rate = %f \" %TNR)\n",
    "print(\"Misclassification Error = %f \" %Err)\n",
    "print(\"Accuracy = %f \" %Acc)\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "cf_matrix_value = [TNR,FPR,FNR,TPR]\n",
    "\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix_value]\n",
    "\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "ax.set_title('Decision Tree Classifier Confusion Matrix\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded65c5",
   "metadata": {},
   "source": [
    "### 2) Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning RFC\n",
    "RFC = RandomForestClassifier()\n",
    "parames_RFC = [{ 'n_estimators': [50,100,150, 200, 250, 300]}]\n",
    "RFC_tuned = GridSearchCV(RFC, parames_RFC, n_jobs = -1, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "RFC_tuned.fit(X_train, y_train)\n",
    "print(\"Accuracy of RFC model:\",accuracy_score(y_test, RFC_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1fc22",
   "metadata": {},
   "source": [
    "**Notes:** A tuned Random Forest Classifier gives a lower accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5736d",
   "metadata": {},
   "source": [
    "### 3) Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "scale = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34592aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split on dataset for Ensemble\n",
    "X_trainEN, X_testEN, y_trainEN, y_testEN = train_test_split(X, y, test_size=0.3, random_state=862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data for Ensemble model\n",
    "X_trainEN_s = scale.fit_transform(X_trainEN)\n",
    "X_testEN_s = scale.transform(X_testEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning SVC\n",
    "SVM = SVC(kernel = 'rbf', random_state = 862)\n",
    "params_SVM = {'C': [0.01, 0.1, 1, 10],\n",
    "             'gamma': [0.1, 0.5, 1, 2, 3, 4]}\n",
    "SVM_tuned = GridSearchCV(SVM, params_SVM, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning XGB\n",
    "XGB = XGBClassifier(n_estimators = 5, random_state = 862)\n",
    "params_XGB = {'n_estimators': [10,20,30,40,50]}\n",
    "XGB_tuned = GridSearchCV(XGB, params_XGB, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053992d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the base learners\n",
    "models = {'SVM':SVM_tuned, 'XGB':XGB_tuned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the blender\n",
    "blender = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ef8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the training data into two parts, one to train the weak learners, another to train the blender\n",
    "X_train_s1, X_train_s2, y_train1, y_train2 = train_test_split(X_trainEN_s, y_train, test_size = 0.5, random_state = 862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the base learners\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_s1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b79110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the blender\n",
    "# Get the prediction\n",
    "ENpredictions = pd.DataFrame() # Set up a dataframe to store the predictions\n",
    "for name, model in models.items():\n",
    "    ENpredictions[name] = model.predict(X_train_s2)\n",
    "\n",
    "# Get the blender\n",
    "scaler_blend = StandardScaler() # Scale the predictions \n",
    "predictions_scale = scaler_blend.fit_transform(ENpredictions)\n",
    "blender.fit(predictions_scale, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce77c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation\n",
    "# First send the data through the weak learners\n",
    "ENpredictions = pd.DataFrame() # Set up a dataframe to store the predictions\n",
    "for name, model in models.items():\n",
    "    ENpredictions[name] = model.predict(X_testEN_s)\n",
    "    \n",
    "# Prediction through the blender, and evaluate\n",
    "predictions_scale = scaler_blend.transform(ENpredictions)\n",
    "print(\"Accuracy of Stacking model:\",(accuracy_score(y_testEN, blender.predict(predictions_scale))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7bcde8",
   "metadata": {},
   "source": [
    "## Part 5: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = [np.mean(grid.predict(X_test) == y_test),accuracy_score(y_test, RFC_tuned.predict(X_test)),accuracy_score(y_testEN, blender.predict(predictions_scale))]\n",
    "clfs = ['Decision Tree','Random Forest','Ensemble Stacking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba11421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the results into a DataFrame\n",
    "comparison = pd.DataFrame(clfs, columns = ['Classifiers'])\n",
    "comparison['Accuracy Scores'] = accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e88152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accuracies of all three predictive models\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1642a8ff",
   "metadata": {},
   "source": [
    "**Why is Decision Tree and Ensemble Stacking's accuracies are the same?**\n",
    "* One of the reasons why the accuracies are the same is our proportion of the majority class. Our majority class which is did not purchase is around 85%, so it's possible that some classifiers just predict the majority class all the time. \n",
    "\n",
    "\n",
    "* This would be visible with precision/recall. True positive rate (TPR) is the recall, which means how good is the ability of this classification model to identify all data points correctly. By looking at the Decision Tree Confusion Matrix above, we can see that the TPR is only 7.6%, very low. This might be that most prediction model are just constantly predict the people who do not purchases instead of the people who purchase. \n",
    "\n",
    "\n",
    "* Despite of this flaw, we were still able to find out many insights. And that leads to our last and final slide about recommendations. \n",
    "\n",
    "\n",
    "*Source: https://datascience.stackexchange.com/questions/101881/all-machine-learning-models-are-giving-the-same-accuracy*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33075f",
   "metadata": {},
   "source": [
    "**Conclusions and Recommendations**: \n",
    "* First, the marketing team should focus on webpage improvement. It is because we know that NewVisitor have high purchasing rate. Therefore, marketing team should focus on making the webpage more attractive and inviting to make a very good first impression. \n",
    "\n",
    "\n",
    "* Findings from Decision Tree Classifier visualization suggest that the most important features to predict a visitor purchase or not are the duration a visitor spend on the webpage, the bounce rate from the number of pages and the November month. If you look at the visualization of decision tree on the right hand side, the branches and nodes help us to interpret who is our ideal visitor segmentation. According to our decision tree, in the past, the visitors who have purchase rate are the people who visit the page on November month, who spend total of 4384 or more of duration on Product_Related pages and have a bounce rate of under 0.011. \n",
    "\n",
    "\n",
    "* Lastly is we found that the month of November has very high purchase rate from the EDA but also the feature importance kinda reinforce that. So the advice for the marketing team is they should utilize November month for as much promotions and advertising campaigns as possible, to encourage people to buy! And that is the end of our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722bb12d",
   "metadata": {},
   "source": [
    "**References:**\n",
    "* https://towardsdatascience.com/are-your-models-using-the-correct-significance-levels-c88367ee0544\n",
    "* https://github.com/krishnaik06/T-test-an-Correlation-using-python/blob/master/Hypothesis_Testing.ipynb\n",
    "* https://towardsdatascience.com/how-to-know-which-statistical-test-to-use-for-hypothesis-testing-744c91685a5d\n",
    "* https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/\n",
    "* https://support.google.com/analytics/answer/2695658?hl=en#:~:text=Page%20Value%20is%20the%20average,more%20to%20your%20site's%20revenue\n",
    "* https://support.google.com/analytics/answer/2525491?hl=en#:~:text=For%20all%20pageviews%20to%20the,that%20start%20with%20that%20page"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
